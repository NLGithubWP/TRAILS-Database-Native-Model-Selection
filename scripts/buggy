



# xai
#NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4

# panda x2
#NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2



# multiple process share the same tensor reports:
    "Cannot re-initialize CUDA in forked subprocess. To use CUDA with "
    RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
    Process Process-12:


# use the queue in torch.mulitplecessing
    n = write(self._handle, buf)
    BrokenPipeError: [Errno 32] Broken pipe


# bottnecked.
When multiple train-jobs uses the same dataloader, It's working but very slow.
Multiple reasons:
    1. Data loading can become a bottleneck: In the case of multiple training jobs
        sharing a single DataLoader, each job needs to wait its turn to fetch a
        batch of data from the queue. This can introduce additional waiting time for
         each job, leading to slower training overall. On the other hand, when each
         training job has its own DataLoader, it can fetch data independently,
          without waiting for other jobs to finish.

    2. Resource contention: When multiple training jobs share a single DataLoader,
       there can be contention for system resources, such as CPU and memory.
       This can happen when multiple jobs try to access the same data at the same time,
       leading to slower performance.

    3. Communication overhead: Sharing a DataLoader using torch multiprocessing queue
       requires communication between processes, which can introduce additional
       overhead. This overhead can be more significant when there are many jobs
       running in parallel, leading to slower performance.


# manually partition and launch multiple train jobs.
    1. To slow to decode the dataset:
        => pre-decode, save /load with torch.save and torch.load,
        => (4 mins to load)

    2. To much memory usage:
        6 worker using 116GB,
        => reduce the memory usage

    3. To slow to do the validation.
        => why is this happening ?
        Is it due to multiple jobs on a single GPU ?
        => try with one train job one GPU on pandax1. This is still not working.
        Finding:
        => To many tasks using to much memory, leading to slow training process of each process.
        The reason may be CPU/Memory access contention,
    4. increase batch-size do not increase the validation speed in a large extends,
        small iterations, while each with more time usage.

    5. New research problem? How to provide a share data loader to multiple train-jobs while
       guarantee the speedups.

   6. Loss is to large, and valid AUC is to bad, why?
      => big model cannot get the ground-truth under the same training configurations such as epoch, iteratnions etc.
      => large model needs more iterations or training.


# The exposing problems:
    1. Why launching so many tasks cause the server lose the NVDA driver?
    2. Why
    


# torch function mismatch
    1. register_full_backward_hook torch1.8
    2. register_full_backward_hook torch1.5